{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "310dae95-6061-4281-8987-32230b920ebe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T05:33:26.132475Z",
     "iopub.status.busy": "2026-01-02T05:33:26.132191Z",
     "iopub.status.idle": "2026-01-02T05:33:26.142646Z",
     "shell.execute_reply": "2026-01-02T05:33:26.140585Z",
     "shell.execute_reply.started": "2026-01-02T05:33:26.132452Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "import logging\n",
    "from collections.abc import Iterable, Iterator\n",
    "#from heapq import heapify_max, heappush_max, heappop_max  # requires python3.14\n",
    "from heapq import heapify, heappush, heappop  # before python3.14\n",
    "import math\n",
    "from itertools import chain\n",
    "#from .log import get_logger  # logging logic in local module\n",
    "#from .pretokenizer import PRE_TOKENIZE_PAT\n",
    "import regex as re\n",
    "\n",
    "\n",
    "#log = get_logger(\"bpe\", level=logging.DEBUG)\n",
    "\n",
    "UTF8 = \"utf8\"\n",
    "\n",
    "PRE_TOKENIZE_PAT = re.compile(\n",
    "    r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    ")\n",
    "\n",
    "def count_pretokens(txt: str, counter: Counter[str]):\n",
    "    \"Pretokenize txt and count pretokens w/ counter.\"\n",
    "    # makes no sense to consider overlapped matches as it will produce\n",
    "    # tons of duplicated pre-tokens.\n",
    "    for m in re.finditer(PRE_TOKENIZE_PAT, txt):\n",
    "        counter[txt[m.start() : m.end()]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d7ca849-a66f-409a-9a54-e753aaf19623",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T17:28:48.938125Z",
     "iopub.status.busy": "2026-01-01T17:28:48.937753Z",
     "iopub.status.idle": "2026-01-01T17:28:48.944949Z",
     "shell.execute_reply": "2026-01-01T17:28:48.943530Z",
     "shell.execute_reply.started": "2026-01-01T17:28:48.938095Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(PRE_TOKENIZE_PAT.pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47f01c88-b896-4cb0-adc2-bacec2a5d5b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T03:49:18.655479Z",
     "iopub.status.busy": "2025-12-29T03:49:18.655138Z",
     "iopub.status.idle": "2025-12-29T03:49:18.662726Z",
     "shell.execute_reply": "2025-12-29T03:49:18.661067Z",
     "shell.execute_reply.started": "2025-12-29T03:49:18.655449Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b't', b'h')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = { (b't', b'h'): 3, (b'st', 't'): 3, (b'q', b' '): 2 }\n",
    "\n",
    "max(d, key=lambda p: PairCountSortKey(pair=p, cnt=d[p]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bf89cf13-7817-43e4-adba-362af58b9804",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T07:18:55.436594Z",
     "iopub.status.busy": "2025-12-29T07:18:55.436284Z",
     "iopub.status.idle": "2025-12-29T07:18:55.450026Z",
     "shell.execute_reply": "2025-12-29T07:18:55.447899Z",
     "shell.execute_reply.started": "2025-12-29T07:18:55.436567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['foo'] []\n"
     ]
    }
   ],
   "source": [
    "d = {}\n",
    "v = d.get('k', [])\n",
    "v.append('foo')\n",
    "v1 = d.get('bar', [])\n",
    "print(v, v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d79890a9-4307-4958-a596-be4edefa1f06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T07:17:54.979797Z",
     "iopub.status.busy": "2025-12-29T07:17:54.979482Z",
     "iopub.status.idle": "2025-12-29T07:17:54.993264Z",
     "shell.execute_reply": "2025-12-29T07:17:54.990695Z",
     "shell.execute_reply.started": "2025-12-29T07:17:54.979771Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function get:\n",
      "\n",
      "get(key, default=None, /) method of builtins.dict instance\n",
      "    Return the value for key if key is in the dictionary, else default.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(d.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d20fc65-78ed-4d4a-9e95-e7d89ebf068f",
   "metadata": {},
   "source": [
    "# Scratch: Baseline BPE & WIP optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8706f7e2-14a7-4a33-afc9-1ee6ddcbfec7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T16:44:27.742774Z",
     "iopub.status.busy": "2026-01-01T16:44:27.742439Z",
     "iopub.status.idle": "2026-01-01T16:44:27.760628Z",
     "shell.execute_reply": "2026-01-01T16:44:27.759013Z",
     "shell.execute_reply.started": "2026-01-01T16:44:27.742744Z"
    }
   },
   "outputs": [],
   "source": [
    "class BpeIterationStates:\n",
    "    \"\"\"\n",
    "    Manage the state of a single BPE iteartion run. It does following:\n",
    "        - Keeps the mapping from each token pair found to following:\n",
    "            - the pair's running count, to find out the final pair to create\n",
    "              new token.\n",
    "            - Set of pre-token(s) where the pair is found. To efficiently\n",
    "              update the mapping of pre-tokens to their count to reflect the\n",
    "              presence of new token after its creation.\n",
    "        - Keeps the most frequent token pair(s) found during a single BPE\n",
    "          iteration run. To find the final pair to create new token.\n",
    "\n",
    "    (IMO the pretoken -> count mapping shall be part of this state as well,\n",
    "    esp. to adopt the bpe optimization idea in assignment)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.counter: Counter[tuple[bytes, bytes]] = Counter()\n",
    "        self.pair_to_pretokens: DefaultDict[\n",
    "            tuple[bytes, bytes], set[tuple[bytes, ...]]\n",
    "        ] = defaultdict(set)\n",
    "        self.max_cnt = 0\n",
    "        self.most_pairs: list[tuple[bytes, bytes]] = []\n",
    "\n",
    "    def update(\n",
    "        self, pair: tuple[bytes, bytes], pretoken: tuple[bytes, ...], pretoken_cnt: int\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Update the state w/ given the token pair, the pretoken where the pair is\n",
    "        found, and the count of pretoken in text corpus.\n",
    "\n",
    "        Spec:\n",
    "            Increase the pair's running count by pretoken_cnt.\n",
    "            Save pair -> pretoken mapping to pair_to_pretokens.\n",
    "            Compare the pair's running cnt w/ self.max_cnt:\n",
    "            - If cnt < self.max_cnt, nop.\n",
    "            - If cnt == self.max_cnt, append pair to self.most_pairs.\n",
    "            - If cnt > self.max_cnt, set self.max_cnt = cnt and set\n",
    "                self.most_pairs to only contain pair.\n",
    "        \"\"\"\n",
    "        self.counter[pair] += pretoken_cnt\n",
    "        self.pair_to_pretokens[pair].add(pretoken)\n",
    "        cnt = self.counter[pair]\n",
    "        if cnt == self.max_cnt:\n",
    "            self.most_pairs.append(pair)\n",
    "        elif cnt > self.max_cnt:\n",
    "            self.most_pairs = [pair]\n",
    "            self.max_cnt = cnt\n",
    "\n",
    "    def pair_to_merge(self) -> tuple[bytes, bytes] | None:\n",
    "        \"\"\"\n",
    "        Returns pair of highest count and highest lexical order, or None if no\n",
    "        pair found. Only call this after BPE iteration run finishes.\n",
    "        \"\"\"\n",
    "        return max(self.most_pairs, default=None)\n",
    "\n",
    "\n",
    "def bpe_baseline(\n",
    "    pretokens: dict[str, int], vocab_size: int, special_tokens: list[str]\n",
    ") -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "    - UTs\n",
    "\n",
    "    Train a BPE tokenizer from pre-tokenized input.\n",
    "\n",
    "    Spec:\n",
    "        1. Read pre-tokenized data from input_path, which is in form of a json\n",
    "            object whose key is pre-token and value is the pre-token's frequency in\n",
    "            text corpus, into a dict[tuple[bytes], int]\n",
    "        2. Initialize vocabulary as a dict[int, bytes], w/ int key 0 - 255 maps to\n",
    "            numerically identical 1-byte bytes value. Then assign new IDs\n",
    "            starting from 256 for each given special token and add\n",
    "            corresponding mapping to vocabulary. Initialize list[tuple[bytes,\n",
    "            bytes]] to store merges resulted from BPE.\n",
    "\n",
    "            At this point:\n",
    "                1. id for next new token = 256 + len(special_tokens).\n",
    "                2. # remaining available slots in vocabulary = vocab_size - id_of_next_new_token\n",
    "        3. Build up vocabulary by running BPE till vocabular size hits\n",
    "            vocab_size iteratively. Start iteration from pairing adjacent byte-level\n",
    "            tokens present in each pre token (indiviaul key from step 1). Use\n",
    "            a Counter to manage the running frequency count of each pair found. Each\n",
    "            iteration is to find the token pair w/ the highest frequency, and\n",
    "            when there are multiple pairs w/ the same highest frequency, break\n",
    "            tie by choosing the pair w/ highest lexico order. Specifically:\n",
    "                For each pre-token pt:\n",
    "                    Start from byte index 0, pick pair of adjacent byte-level\n",
    "                    tokens and increment its count in Counter.\n",
    "                    Need a way to record the mapping from a pair to pre-tokens\n",
    "                    which contain such pair, so that later on we can find these\n",
    "                    pre-tokens in O(1) instead of blindly visiting all\n",
    "                    pre-tokens. Need a mapping dict[bytes, set[pre-tokens that\n",
    "                    contains the pair]]\n",
    "\n",
    "            For efficiency, Counter doesn't\n",
    "            have an eas way to get keys whose count is the highest so will need\n",
    "            new logic to track those keys if we want to avoid repeatedly\n",
    "            sorting Counter's entries to find them out. Must to track all pairs\n",
    "            w/ highest frequency as future pairs of highest frequency are built\n",
    "            on top of them.\n",
    "\n",
    "            Once we find the pair of highest freq and break the tie, give it a\n",
    "            new ID and add it to vocabulary, also add the pair to merges list.\n",
    "\n",
    "    \"\"\"\n",
    "    token_seq_cnts = {\n",
    "        tuple(bytes([b]) for b in pt.encode(UTF8)): cnt for pt, cnt in pretokens.items()\n",
    "    }\n",
    "    vocab = {i: bytes([i]) for i in range(256)}\n",
    "    merges: list[tuple[bytes, bytes]] = []\n",
    "    for t in special_tokens:\n",
    "        # len(vocab) is the id of next new token\n",
    "        new_token_id = len(vocab)\n",
    "        vocab[new_token_id] = t.encode(UTF8)\n",
    "\n",
    "    # NOTE for bpe we only concern about tokens created from merging existing\n",
    "    # ones.\n",
    "    while len(vocab) < vocab_size:\n",
    "        state = BpeIterationStates()\n",
    "        for tokens, pretoken_cnt in token_seq_cnts.items():\n",
    "            # Ignore 1-byte pretoken\n",
    "            if len(tokens) == 1:\n",
    "                continue\n",
    "            # Current pretoken contain > 1 tokens. Iterate each token and\n",
    "            # collect pair of itself and its successor as merge candidate\n",
    "            for p in zip(tokens, tokens[1:]):\n",
    "                state.update(p, tokens, pretoken_cnt)\n",
    "\n",
    "        p_to_merge = state.pair_to_merge()\n",
    "        if p_to_merge is None:\n",
    "            log.warn(\"Pair to merge is not found. This shall not happen!\")\n",
    "            break\n",
    "        new_token = b\"\".join(p_to_merge)\n",
    "        # NOTE check whether the new token already existed in vocab?\n",
    "        new_token_id = len(vocab)\n",
    "        vocab[new_token_id] = new_token\n",
    "        merges.append(p_to_merge)\n",
    "        print(\n",
    "            f\"Merging pair {p_to_merge} of count {state.counter[p_to_merge]} to new token {new_token_id}\"\n",
    "        )\n",
    "        # update token_seq_cnts to reflect the presence of new token\n",
    "        pretokens_to_update = state.pair_to_pretokens[p_to_merge]\n",
    "        for pretoken in pretokens_to_update:\n",
    "            # keep its count in token_seq_cnts\n",
    "            cnt = token_seq_cnts.pop(pretoken)\n",
    "            # replace all non-overlapping occurrences of token pair in pretoken w/ new token\n",
    "            idx, ln, updated_pretoken = 0, len(pretoken), []\n",
    "            while idx < ln:\n",
    "                if (\n",
    "                    idx < ln - 1\n",
    "                    and pretoken[idx] == p_to_merge[0]\n",
    "                    and pretoken[idx + 1] == p_to_merge[1]\n",
    "                ):\n",
    "                    updated_pretoken.append(new_token)\n",
    "                    idx += 2\n",
    "                else:\n",
    "                    updated_pretoken.append(pretoken[idx])\n",
    "                    idx += 1\n",
    "            # save mapping updated_pretoken -> cnt back to token_seq_cnts\n",
    "            token_seq_cnts[tuple(updated_pretoken)] = cnt\n",
    "\n",
    "    return vocab, merges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3732c562-c936-4aa9-a656-0161a138165f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Functional BPE impl expected to be more efficient than baseline, but only provided mediocre speedup\n",
    "\n",
    "Due to bottleneck of blindly repeatedly sorting all known pair counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c2c3fc5-71cd-423e-9850-f114dc31d67a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T23:21:58.882362Z",
     "iopub.status.busy": "2025-12-31T23:21:58.877098Z",
     "iopub.status.idle": "2025-12-31T23:21:58.932423Z",
     "shell.execute_reply": "2025-12-31T23:21:58.929315Z",
     "shell.execute_reply.started": "2025-12-31T23:21:58.881579Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Correct, functional impl of time-optimized BPE.\n",
    "'''\n",
    "def bpe_time_suboptimal(\n",
    "    pretokens: dict[str, int], vocab_size: int, special_tokens: list[str]\n",
    ") -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n",
    "\n",
    "    # Need a mapping from pretoken str -> pretoken's tokens. NOTE the value can change\n",
    "    # as we create new merged tokens w/ BPE.\n",
    "    class Pretoken:\n",
    "        def __init__(self, seq, cnt):\n",
    "            '''\n",
    "            seq: Token sequence representing the pretoken.\n",
    "            cnt: Count of pretoken in corpus.\n",
    "            '''\n",
    "            self.seq = seq\n",
    "            self.cnt = cnt\n",
    "\n",
    "        def __repr__(self) -> str:\n",
    "            return str((self.seq, self.cnt))\n",
    "\n",
    "    pretoken_info: dict[str, Pretoken] = {}\n",
    "    for pt, cnt in pretokens.items():\n",
    "        seq = tuple(bytes([b]) for b in pt.encode(UTF8))\n",
    "        # BPE merge needs at least 2 tokens\n",
    "        if len(seq) < 2:\n",
    "            continue\n",
    "        pretoken_info[pt] = Pretoken(seq, cnt)\n",
    "\n",
    "    vocab = {i: bytes([i]) for i in range(256)}\n",
    "    merges: list[tuple[bytes, bytes]] = []\n",
    "    for t in special_tokens:\n",
    "        # len(vocab) is the id of next new token\n",
    "        new_token_id = len(vocab)\n",
    "        vocab[new_token_id] = t.encode(UTF8)\n",
    "\n",
    "    # NOTE for bpe we only concern about tokens created from merging existing ones.\n",
    "    pair_cnts: Counter[tuple[bytes, bytes]] = Counter()\n",
    "    # token pair -> pretokens which contain such pair\n",
    "    pair_to_pretokens: DefaultDict[tuple[bytes, bytes], set[str]] = (\n",
    "        defaultdict(set)\n",
    "    )\n",
    "\n",
    "    # iterative BPE runs\n",
    "    merged_p: tuple[bytes, bytes] = None\n",
    "    while len(vocab) < vocab_size:\n",
    "        if merged_p is None:\n",
    "            # Initial condition: Haven't identify any merged token\n",
    "            # run a full pass of pretokens to collect initial byte-token pairs\n",
    "            for pt, v in pretoken_info.items():\n",
    "                # Assumption: pt contains > 1 tokens. Iterate each token and\n",
    "                # collect pair of itself and its successor as merge candidate\n",
    "                for p in zip(v.seq, v.seq[1:]):\n",
    "                    pair_cnts[p] += v.cnt # CHIU increment by pretoken's count instead of 1!\n",
    "                    pair_to_pretokens[p].add(pt)\n",
    "        elif len(pair_cnts) == 0:\n",
    "            # Unlikely to happen; But if this was true then it means there are no more\n",
    "            # new merged token to be created. So log and exit loop\n",
    "            print(f'Cannot merge further as token pairs have run out. Actual vocab size: {len(vocab)} Expected: {vocab_size}')\n",
    "            break\n",
    "        else:\n",
    "            # Previous iteration has identified a merged token\n",
    "            # Find candidates of the merged pair (to be identified in current iteration),\n",
    "            # which can be:\n",
    "            # - A pair already in pair_cnts, OR\n",
    "            # - A new pair resulted from creation of merged token from previous iteration\n",
    "            # and update mapping pair -> count and pair -> pretokens accordingly\n",
    "            for pt in pair_to_pretokens[merged_p]:\n",
    "                v = pretoken_info[pt]\n",
    "                updated_token_seq(pt, v, merged_p, pair_cnts, pair_to_pretokens)\n",
    "            # Drop merged_p's entry from pair_to_pretoken as it is longer needed\n",
    "            pair_to_pretokens.pop(merged_p)\n",
    "\n",
    "        # FIXME: profiling w/ test data from UT shows sorting logic below is the efficiency bottleneck.\n",
    "        merged_p = max(pair_cnts, key=lambda p: PairCountSortKey(pair=p, cnt=pair_cnts[p]))        \n",
    "        merged_token = b''.join(merged_p)\n",
    "        new_token_id = len(vocab)\n",
    "        vocab[new_token_id] = merged_token\n",
    "        merges.append(merged_p)\n",
    "        print(f\"Merging pair {merged_p} of count {pair_cnts[merged_p]} to new token {new_token_id}\")\n",
    "        debug_pair_to_pretoken_info = { p: [pretoken_info[pt] for pt in pts] for p, pts in pair_to_pretokens.items() }\n",
    "        #print(f'DEBUG: pair_cnts = {pair_cnts}\\npair_to_pretokens = {debug_pair_to_pretoken_info}')\n",
    "        # Now remove the merged pair from pair_cnts to clear way for next merged pair\n",
    "        pair_cnts.pop(merged_p)\n",
    "        \n",
    "    return vocab, merges\n",
    "\n",
    "def updated_token_seq(\n",
    "    pt: str,\n",
    "    ptv: Pretoken,\n",
    "    pair: tuple[bytes, bytes],\n",
    "    pair_cnts: Counter[tuple[bytes, bytes]],\n",
    "    pair_to_pretokens: DefaultDict[tuple[bytes, bytes], set[str]],\n",
    "):\n",
    "    '''\n",
    "    Create updated token sequence of a pretoken given\n",
    "    its existing token sequence and the token pair to merge.\n",
    "\n",
    "    This is to reflect the merge in the pretoken, the implication is that\n",
    "    pair(s) which previously overlap w/ pair to merge in pretoken are now\n",
    "    gone due to the merge, thus we must decrement their count accordingly\n",
    "\n",
    "    (In this light, using a heap to manage pair counts is more awkward as\n",
    "    update to items in heap are not straightforward -- one needs to find\n",
    "    the item, pop it out of heap, update count then push it back. So drop\n",
    "    such idea)\n",
    "\n",
    "    We have to save the count of *new* pairs resulted from merging the merged token to pair_cnts!\n",
    "\n",
    "    pt: Pretoken string\n",
    "    ptv: `Pretoken` value contain pt's token sequence and corpus count \n",
    "    pair: Token pair to merge.\n",
    "    pair_cnts: Token pair counter.\n",
    "    '''\n",
    "    # replace all non-overlapping occurrences of token pair w/ new token\n",
    "    old = ptv.seq\n",
    "    idx, ln = 0, len(old)\n",
    "    new = []\n",
    "    merged_token =  b''.join(pair)\n",
    "    #print(f'DEBUG: updating token sequence - merged pair {pair} pretoken str: \"{pt}\" token seq: {old}')\n",
    "    while idx < ln:\n",
    "        if (\n",
    "            idx < ln - 1\n",
    "            and old[idx] == pair[0]\n",
    "            and old[idx + 1] == pair[1]\n",
    "        ):\n",
    "            new.append(merged_token)\n",
    "            # Find overlapping pairs and update their counts:\n",
    "            # (old[idx-1], old[idx]) and (old[idx+1], old[idx+2])\n",
    "            # Also record token(s) which can be built by merging\n",
    "            # the merged token and its neighbor.\n",
    "            # NOTE!!! Here pair count increments/decrements by\n",
    "            # pretoken's corpus count, not 1\n",
    "            if idx-1 >= 0:\n",
    "                p_gone = (old[idx-1], old[idx])\n",
    "                if p_gone in pair_cnts:\n",
    "                    pair_cnts[p_gone] -= ptv.cnt\n",
    "                    if pair_cnts[p_gone] == 0:\n",
    "                        pair_cnts.pop(p_gone)\n",
    "                \n",
    "                new_p_w_merge_token = (old[idx-1], merged_token)\n",
    "                pair_cnts[new_p_w_merge_token] += ptv.cnt\n",
    "                pair_to_pretokens[new_p_w_merge_token].add(pt)\n",
    "                #print(f'DEBUG: new pair w/ merged token {new_p_w_merge_token} - merged token {merged_token}')\n",
    "                \n",
    "            if idx+2 < ln:\n",
    "                p_gone = (old[idx+1], old[idx+2])\n",
    "                if p_gone in pair_cnts:\n",
    "                    pair_cnts[p_gone] -= ptv.cnt\n",
    "                    if pair_cnts[p_gone] == 0:\n",
    "                        pair_cnts.pop(p_gone)\n",
    "                \n",
    "                new_p_w_merge_token = (merged_token, old[idx+2])\n",
    "                pair_cnts[new_p_w_merge_token] += ptv.cnt\n",
    "                pair_to_pretokens[new_p_w_merge_token].add(pt)\n",
    "                #print(f'DEBUG: new pair w/ merged token {new_p_w_merge_token} - merged token {merged_token}')\n",
    "                \n",
    "            idx += 2\n",
    "        else:\n",
    "            new.append(old[idx])\n",
    "            idx += 1\n",
    "\n",
    "    ptv.seq = new\n",
    "\n",
    "\n",
    "class PairCountSortKey:\n",
    "    '''\n",
    "    Sort key to find token pair of highest count and largest lexical order.\n",
    "\n",
    "    NOTE this is a useful way to encapsulate complex comparison logic which\n",
    "    cannot fit into a one-liner lambda function:\n",
    "\n",
    "    Suppose pairs is a list of token pairs.\n",
    "    Before:\n",
    "    sorted(pairs, key=lambda p: # cannot fit in logic to first compare count then lexical order! ...)\n",
    "    After:\n",
    "    sorted(pairs, key=lambda p: PairCountSortKey(pair=p, cnt=pair_cnts[pair]))\n",
    "    '''\n",
    "    def __init__(self, pair: tuple[bytes, bytes], cnt: int) -> None:\n",
    "        self.pair = pair\n",
    "        self.cnt = cnt\n",
    "\n",
    "    def __lt__(self, other: \"PairCountSortKey\") -> bool:\n",
    "        \"\"\"\n",
    "        https://docs.python.org/3/reference/datamodel.html#object.__lt__\n",
    "\n",
    "        This pair is deemed less than other if it has a lowr count, or it is\n",
    "        lexically smaller when there is a tie on count.\n",
    "\n",
    "        This seems to work w/ max() too as long as two values in comparison\n",
    "        is of same type. See https://stackoverflow.com/a/72880603\n",
    "        \"\"\"\n",
    "        if self.cnt != other.cnt:\n",
    "            return self.cnt < other.cnt\n",
    "        # A tie on count; Break it by lexical ordering\n",
    "        return self.pair < other.pair\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return str((self.pair, self.cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799e2327-f5ae-4e5b-8345-9d55e610717e",
   "metadata": {},
   "source": [
    "# Correctly optimized BPE impl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfc60965-e2ff-437a-85c4-2aab9d36d7d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T01:37:47.776120Z",
     "iopub.status.busy": "2026-01-02T01:37:47.775792Z",
     "iopub.status.idle": "2026-01-02T01:37:47.815528Z",
     "shell.execute_reply": "2026-01-02T01:37:47.813998Z",
     "shell.execute_reply.started": "2026-01-02T01:37:47.776090Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Functional, truely optimized and performant BPE algo implementation.\n",
    "'''\n",
    "\n",
    "class Pretoken:\n",
    "    def __init__(self, seq, cnt):\n",
    "        \"\"\"\n",
    "        seq: Token sequence representing the pretoken.\n",
    "        cnt: Count of pretoken in corpus.\n",
    "        \"\"\"\n",
    "        self.seq = seq\n",
    "        self.cnt = cnt\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return str((self.seq, self.cnt))\n",
    "\n",
    "\n",
    "def bpe_time_optimized(\n",
    "    pretokens: dict[str, int], vocab_size: int, special_tokens: list[str]\n",
    ") -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n",
    "    \"\"\"\n",
    "    Start: Make a full pass to all pretokens and get byte-token pair -> count\n",
    "        mapping. This also yields the 1st merged token, aka the byte-token pair\n",
    "        w/ the highest count. Denote this pair as p1.\n",
    "\n",
    "    In byte-token pair -> count mapping, find pairs which are not p1 but\n",
    "    overlap w/ p1; Suppose pair p meets such criteria, then it means either of\n",
    "    following is true:\n",
    "    - p[0] = p1[1], e.g. (b't', b'h') and (b' ', b't')\n",
    "    - p[1] = p1[0], e.g. (b't', b'h') and ('h', 'e')\n",
    "\n",
    "    NOTE such property is DIFFERENT from that p shares 1 common token w/\n",
    "    p1, as the latter case implies there exist cases e.g. p[0] = p1[0] or p[1]\n",
    "    = p1[1]. However we cannot merge p and p1 to create a new token -- merge is\n",
    "    only possible when we can concatenate token over the overlapped element,\n",
    "    e.g. merge p = (b't', b'h') and p1 = b' t' (previously merged token)\n",
    "    yields pair (b' t', b'h') and token b' th'\n",
    "\n",
    "    To determine the 2nd merged token, BPE requires us to find\n",
    "    corresponding token pair w/ the highest count in text corpus; Note such\n",
    "    pair may or may not overlap w/ p1.\n",
    "    (as we have seen in `TinyStoriesV2-GPT4-valid.txt.pretokens.json`)\n",
    "\n",
    "    IMO the optimization idea mentioned by assignment tries relying on the\n",
    "    hypothesis that tokens created by merging a already merged token and one\n",
    "    which has overlaps with it will have\n",
    "    higher count compared to picking up pair of random adjacent token and count\n",
    "    the pair's occurrence, hence the efficiency increase. As mentioned above,\n",
    "    such hypothesis can be false, which makes us find out the pair that has\n",
    "    the 2nd highest count but doesn't overlap w/ p1.\n",
    "\n",
    "    Idea to remove the bottleneck: Use a max heap to track the next merged\n",
    "    pair. Item in the heap will be of PairCount type, and we will use\n",
    "    PairCount.cnt < 0 to signal that an item is no longer valid because\n",
    "    the corresponding pair's count has changed. We will rely on keep pushing\n",
    "    items of updated pair-cnt and that for new pairs to maintain max heap's\n",
    "    correctness. This is not very ideal (aka tracking the next merged pair\n",
    "    which has highest count and lexical order in O(1) time), but at least we no\n",
    "    longer need to sort all the known pair-cnt entries (O(nlgn)) - we will only\n",
    "    need 2 x |pairs w/ updated count| + |new pairs| heap push/pop operations,\n",
    "    (where 2x is due to popping invalid pairs and pushing updated pairs)\n",
    "    resulting time complexity of O((2 x |pairs w/ updated count| + |new pairs|)lgn)\n",
    "    \"\"\"\n",
    "\n",
    "    # Need a mapping from pretoken str -> token sequence which forms the\n",
    "    # pretoken. Because the token sequence can change as we identify token pair\n",
    "    # to merge and create new merged token from the pair.\n",
    "    # To facilitate access and update of pretoken's token sequence and\n",
    "    # pretoken's count, we need a \"container\" type, hence the Pretoken definition\n",
    "    pretoken_info: dict[str, Pretoken] = {}\n",
    "    for pt, cnt in pretokens.items():\n",
    "        seq = tuple(bytes([b]) for b in pt.encode(UTF8))\n",
    "        # BPE merge needs at least 2 tokens\n",
    "        if len(seq) < 2:\n",
    "            continue\n",
    "        pretoken_info[pt] = Pretoken(seq, cnt)\n",
    "\n",
    "    vocab = {i: bytes([i]) for i in range(256)}\n",
    "    merges: list[tuple[bytes, bytes]] = []\n",
    "    for t in special_tokens:\n",
    "        # len(vocab) is the id of next new token\n",
    "        new_token_id = len(vocab)\n",
    "        vocab[new_token_id] = t.encode(UTF8)\n",
    "\n",
    "    pair_cnts: dict[tuple[bytes, bytes], PairCount] = {}\n",
    "    # token pair -> pretokens which contain such pair\n",
    "    pair_to_pretokens: DefaultDict[tuple[bytes, bytes], set[str]] = defaultdict(set)\n",
    "    # PairCount max heap\n",
    "    pch: list[PairCount] = []\n",
    "\n",
    "    # iterative BPE runs\n",
    "    merged_p: tuple[bytes, bytes] | None = None\n",
    "    while len(vocab) < vocab_size:\n",
    "        if merged_p is None:\n",
    "            # Initial condition: Haven't identify any merged token\n",
    "            # run a full pass of pretokens to collect initial byte-token pairs\n",
    "            for pt, v in pretoken_info.items():\n",
    "                # Assumption: pt contains > 1 tokens. Iterate each token and\n",
    "                # collect pair of itself and its successor as merge candidate\n",
    "                for p in zip(v.seq, v.seq[1:]):\n",
    "                    pc = pair_cnts.get(p, PairCount(p, 0))\n",
    "                    pc.cnt += v.cnt  # NOTE change in count is not 1!\n",
    "                    pair_cnts[p] = pc\n",
    "                    pair_to_pretokens[p].add(pt)\n",
    "            # Build a max heap from created PairCount values\n",
    "            pch = list(pair_cnts.values())\n",
    "            heapify_max(pch)\n",
    "        elif len(pair_cnts) == 0:\n",
    "            # Unlikely to happen; But if this was true then it means there are no more\n",
    "            # new merged token to be created. So log and exit loop\n",
    "            print(\n",
    "                f\"Cannot merge further as token pairs have run out. Actual vocab size: {len(vocab)} Expected: {vocab_size}\"\n",
    "            )\n",
    "            break\n",
    "        else:\n",
    "            # Previous iteration has identified a merged token\n",
    "            # Find candidates of the merged pair (to be identified in current iteration),\n",
    "            # which can be:\n",
    "            # - A pair already in pair_cnts, OR\n",
    "            # - A new pair resulted from creation of merged token from previous iteration\n",
    "            # and update mapping pair -> count and pair -> pretokens accordingly\n",
    "            proto_update_token_seqs(merged_p, pair_to_pretokens, pretoken_info, pair_cnts, pch)\n",
    "            #for pt in pair_to_pretokens[merged_p]:\n",
    "            #    v = pretoken_info[pt]\n",
    "            #    updated_token_seq(pt, v, merged_p, pair_cnts, pair_to_pretokens)\n",
    "            # Drop merged_p's entry from pair_to_pretoken as it is longer needed\n",
    "            pair_to_pretokens.pop(merged_p)\n",
    "            if not pch:\n",
    "                print(\n",
    "                    f\"Cannot merge further as token pairs have run out. Actual vocab size: {len(vocab)} Expected: {vocab_size}\"\n",
    "                )\n",
    "                break\n",
    "\n",
    "        # Q: How can we ensure the top of hcp at this point refers to the\n",
    "        # merged pair found by the current iteration?\n",
    "        #print(f'DEBUG: heap before removing the merged pair: {pch}')\n",
    "        merged_pc = heappop_max(pch)\n",
    "        merged_p = merged_pc.pair\n",
    "        merged_token = b\"\".join(merged_p)\n",
    "        new_token_id = len(vocab)\n",
    "        vocab[new_token_id] = merged_token\n",
    "        merges.append(merged_p)\n",
    "        print(\n",
    "            f\"Merging pair {merged_p} of count {pair_cnts[merged_p].cnt} to new token {new_token_id}\"\n",
    "        )\n",
    "        # NOTE comment out following and test speed again\n",
    "        #debug_pair_to_pretoken_info = {\n",
    "        #    p: [pretoken_info[pt] for pt in pts] for p, pts in pair_to_pretokens.items()\n",
    "        #}\n",
    "        # print(f'DEBUG: pair_cnts = {pair_cnts}\\npair_to_pretokens = {debug_pair_to_pretoken_info}')\n",
    "        # Now remove the merged pair from pair_cnts to clear way for next merged pair\n",
    "        pair_cnts.pop(merged_p)\n",
    "\n",
    "    return vocab, merges\n",
    "\n",
    "def proto_update_token_seqs(\n",
    "        p: tuple[bytes, bytes],\n",
    "        pair_to_pretokens: DefaultDict[tuple[bytes, bytes], set[str]],\n",
    "        pretoken_info: dict[str, Pretoken],\n",
    "        pair_cnts: dict[tuple[bytes, bytes], PairCount],\n",
    "        pch: list[PairCount],\n",
    "):\n",
    "    '''\n",
    "    p: merged pair\n",
    "\n",
    "    Spec:\n",
    "    Invalidate PairCount values in pch whose pair's count has been updated,\n",
    "    besides updating other states. After a pair's count has been fully updated,\n",
    "    create a new PairCount value and push it to heap. Same for new pairs\n",
    "    created from merging the merged token.\n",
    "    '''\n",
    "    new_pcs: dict[tuple[bytes, bytes], PairCount] = {}\n",
    "    for pt in pair_to_pretokens[p]:\n",
    "        ptv = pretoken_info[pt]\n",
    "        # replace all non-overlapping occurrences of token pair w/ new token\n",
    "        old = ptv.seq\n",
    "        idx, ln = 0, len(old)\n",
    "        new = []\n",
    "        merged_token = b\"\".join(p)\n",
    "        # print(f'DEBUG: updating token sequence - merged pair {pair} pretoken str: \"{pt}\" token seq: {old}')\n",
    "        while idx < ln:\n",
    "            if idx < ln - 1 and old[idx] == p[0] and old[idx + 1] == p[1]:\n",
    "                new.append(merged_token)\n",
    "                # Find overlapping pairs and update their counts:\n",
    "                # (old[idx-1], old[idx]) and (old[idx+1], old[idx+2])\n",
    "                # Also record token(s) which can be built by merging\n",
    "                # the merged token and its neighbor.\n",
    "                # NOTE!!! Here pair count increments/decrements by\n",
    "                # pretoken's corpus count, not 1\n",
    "                if idx - 1 >= 0:\n",
    "                    p_gone = (old[idx - 1], old[idx])\n",
    "                    if p_gone in pair_cnts:\n",
    "                        # NOTE default value is returned only when we see this\n",
    "                        # pair in the loop for the 1st time.\n",
    "                        pc = new_pcs.get(p_gone, pair_cnts[p_gone].copy())\n",
    "                        pc.cnt -= ptv.cnt\n",
    "                        new_pcs[p_gone] = pc\n",
    "                        # Invalidate the PairCount value from pair_cnts as its\n",
    "                        # count has changed. This will later help us discard\n",
    "                        # invalid PairCount values in the heap. NOTE this will\n",
    "                        # NOT break the loose ordering b/w items in heap; Updating\n",
    "                        # the count value of items in heap will, so avoid it.\n",
    "                        pair_cnts[p_gone].valid = False\n",
    "                        if pc.cnt == 0:\n",
    "                            pair_cnts.pop(p_gone)\n",
    "                            new_pcs.pop(p_gone)\n",
    "                            # TODO optional to pop p_gone from\n",
    "                            # pair_to_pretokens\n",
    "\n",
    "                    new_p_w_merge_token = (old[idx - 1], merged_token)\n",
    "                    new_pc_w_merge_token = new_pcs.get(\n",
    "                            new_p_w_merge_token, PairCount(new_p_w_merge_token, 0))\n",
    "                    new_pc_w_merge_token.cnt += ptv.cnt\n",
    "                    new_pcs[new_p_w_merge_token] = new_pc_w_merge_token\n",
    "                    pair_to_pretokens[new_p_w_merge_token].add(pt)\n",
    "                    # print(f'DEBUG: new pair w/ merged token {new_p_w_merge_token} - merged token {merged_token}')\n",
    "\n",
    "                if idx + 2 < ln:\n",
    "                    p_gone = (old[idx + 1], old[idx + 2])\n",
    "                    if p_gone in pair_cnts:\n",
    "                        pc = new_pcs.get(p_gone, pair_cnts[p_gone].copy())\n",
    "                        pc.cnt -= ptv.cnt\n",
    "                        new_pcs[p_gone] = pc\n",
    "                        pair_cnts[p_gone].valid = False\n",
    "                        if pair_cnts[p_gone] == 0:\n",
    "                            pair_cnts.pop(p_gone)\n",
    "                            new_pcs.pop(p_gone)\n",
    "\n",
    "                    new_p_w_merge_token = (merged_token, old[idx + 2])\n",
    "                    new_pc_w_merge_token = new_pcs.get(\n",
    "                            new_p_w_merge_token, PairCount(new_p_w_merge_token, 0))\n",
    "                    new_pc_w_merge_token.cnt += ptv.cnt\n",
    "                    new_pcs[new_p_w_merge_token] = new_pc_w_merge_token\n",
    "                    pair_to_pretokens[new_p_w_merge_token].add(pt)\n",
    "                    # print(f'DEBUG: new pair w/ merged token {new_p_w_merge_token} - merged token {merged_token}')\n",
    "\n",
    "                idx += 2\n",
    "            else:\n",
    "                new.append(old[idx])\n",
    "                idx += 1\n",
    "\n",
    "        ptv.seq = new\n",
    "    # Now we have collected pair -> count entries for:\n",
    "    # 1. Existent pairs whose count has been decremented (not down to 0)\n",
    "    # 2. New pairs built from merging the merged token\n",
    "    # Now restore the (count and lexical) ordering among pairs by putting\n",
    "    # these entries to pair_cnts and pair count max heap.\n",
    "    for pair, pc in new_pcs.items():\n",
    "        pair_cnts[pair] = pc\n",
    "        heappush_max(pch, pc)\n",
    "    # Finally pop the heap until we see a valid PairCount at heap top, which is\n",
    "    # the next merged pair\n",
    "    while pch and (not pch[0].valid or pch[0].cnt == 0):\n",
    "        heappop_max(pch)\n",
    "\n",
    "class PairCount:\n",
    "    \"\"\"\n",
    "    Sort key to find token pair of highest count and largest lexical order.\n",
    "\n",
    "    NOTE this is a useful way to encapsulate complex comparison logic which\n",
    "    cannot fit into a one-liner lambda function:\n",
    "\n",
    "    Suppose pairs is a list of token pairs.\n",
    "    Before:\n",
    "    sorted(pairs, key=lambda p: # cannot fit in logic to first compare count then lexical order! ...)\n",
    "    After:\n",
    "    sorted(pairs, key=lambda p: PairCount(pair=p, cnt=pair_cnts[pair]))\n",
    "\n",
    "    NOTE we mark a PairCount as invalid by setting its count to < 0.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pair: tuple[bytes, bytes], cnt: int) -> None:\n",
    "        self.pair = pair\n",
    "        self.cnt = cnt\n",
    "        # Check this when we pop a PairCount value from max heap\n",
    "        # Discard the value if it is false. NOTE value of this field\n",
    "        # shall NOT influence a PairCount's ordering in heap -- We\n",
    "        # shall refrain from deliberately violating heap's invariance.\n",
    "        self.valid = True\n",
    "\n",
    "    def __lt__(self, other: \"PairCount\") -> bool:\n",
    "        \"\"\"\n",
    "        https://docs.python.org/3/reference/datamodel.html#object.__lt__\n",
    "\n",
    "        This pair is deemed less than other if it has a lowr count, or it is\n",
    "        lexically smaller when there is a tie on count.\n",
    "\n",
    "        This seems to work w/ max() too as long as two values in comparison\n",
    "        is of same type. See https://stackoverflow.com/a/72880603\n",
    "        \"\"\"\n",
    "        if self.cnt != other.cnt:\n",
    "            return self.cnt < other.cnt\n",
    "        # A tie on count; Break it by lexical ordering\n",
    "        return self.pair < other.pair\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return str((self.pair, self.valid, self.cnt))\n",
    "\n",
    "    def copy(self) -> PairCount:\n",
    "        'Return a copy of itself.'\n",
    "        return PairCount(self.pair, self.cnt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53827454-b87a-419b-b1aa-1bbb93dd7807",
   "metadata": {},
   "source": [
    "# Experiment & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4df742a5-183e-4886-9024-0bdbe53bc64c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T01:37:49.993370Z",
     "iopub.status.busy": "2026-01-02T01:37:49.993045Z",
     "iopub.status.idle": "2026-01-02T01:37:50.001692Z",
     "shell.execute_reply": "2026-01-02T01:37:50.000270Z",
     "shell.execute_reply.started": "2026-01-02T01:37:49.993341Z"
    }
   },
   "outputs": [],
   "source": [
    "txt = '''I've got following inspiring quotes from Ed, the Green Lake legend OG:\n",
    "\n",
    "Life is a testing place not a resting place.\n",
    "\n",
    "Your attidue decides your altitude.\n",
    "\n",
    "Hooping. No begging. Stop running your mouth.\n",
    "\n",
    "End quote.'''\n",
    "#txt = 'aab abac'\n",
    "pretoken_counts = Counter()\n",
    "vocab_size = 300\n",
    "special_tokens = ['<|endoftext|>']\n",
    "\n",
    "count_pretokens(txt, pretoken_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ccd7e11-0a21-407f-a89b-a2dcffdfbfae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T16:53:22.649376Z",
     "iopub.status.busy": "2026-01-01T16:53:22.649060Z",
     "iopub.status.idle": "2026-01-01T16:53:22.662194Z",
     "shell.execute_reply": "2026-01-01T16:53:22.660610Z",
     "shell.execute_reply.started": "2026-01-01T16:53:22.649348Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging pair (b'i', b'n') of count 8 to new token 257\n",
      "Merging pair (b'in', b'g') of count 7 to new token 258\n",
      "Merging pair (b'o', b'u') of count 4 to new token 259\n",
      "Merging pair (b'o', b't') of count 4 to new token 260\n",
      "Merging pair (b'e', b's') of count 4 to new token 261\n",
      "Merging pair (b' ', b'a') of count 4 to new token 262\n",
      "Merging pair (b'ou', b'r') of count 3 to new token 263\n",
      "Merging pair (b'y', b'our') of count 2 to new token 264\n",
      "Merging pair (b'u', b'ot') of count 2 to new token 265\n",
      "Merging pair (b't', b'ing') of count 2 to new token 266\n",
      "Merging pair (b't', b'i') of count 2 to new token 267\n",
      "Merging pair (b't', b'h') of count 2 to new token 268\n",
      "Merging pair (b'q', b'uot') of count 2 to new token 269\n",
      "Merging pair (b'p', b'l') of count 2 to new token 270\n",
      "Merging pair (b'pl', b'a') of count 2 to new token 271\n",
      "Merging pair (b'pla', b'c') of count 2 to new token 272\n",
      "Merging pair (b'plac', b'e') of count 2 to new token 273\n",
      "Merging pair (b'o', b'p') of count 2 to new token 274\n",
      "Merging pair (b'n', b'd') of count 2 to new token 275\n",
      "Merging pair (b'es', b'ting') of count 2 to new token 276\n",
      "Merging pair (b'e', b'g') of count 2 to new token 277\n",
      "Merging pair (b'd', b'e') of count 2 to new token 278\n",
      "Merging pair (b' ', b'your') of count 2 to new token 279\n",
      "Merging pair (b' ', b'r') of count 2 to new token 280\n",
      "Merging pair (b' ', b'quot') of count 2 to new token 281\n",
      "Merging pair (b' ', b'place') of count 2 to new token 282\n",
      "Merging pair (b' ', b'f') of count 2 to new token 283\n",
      "Merging pair (b'w', b'ing') of count 1 to new token 284\n",
      "Merging pair (b'v', b'e') of count 1 to new token 285\n",
      "Merging pair (b'u', b'n') of count 1 to new token 286\n",
      "Merging pair (b'un', b'n') of count 1 to new token 287\n",
      "Merging pair (b'unn', b'ing') of count 1 to new token 288\n",
      "Merging pair (b'u', b'e') of count 1 to new token 289\n",
      "Merging pair (b'u', b'de') of count 1 to new token 290\n",
      "Merging pair (b'ti', b't') of count 1 to new token 291\n",
      "Merging pair (b'tit', b'ude') of count 1 to new token 292\n",
      "Merging pair (b'ti', b'd') of count 1 to new token 293\n",
      "Merging pair (b'tid', b'ue') of count 1 to new token 294\n",
      "Merging pair (b'th', b'e') of count 1 to new token 295\n",
      "Merging pair (b't', b'tidue') of count 1 to new token 296\n",
      "Merging pair (b't', b'op') of count 1 to new token 297\n",
      "Merging pair (b't', b'esting') of count 1 to new token 298\n",
      "Merging pair (b's', b'p') of count 1 to new token 299\n"
     ]
    }
   ],
   "source": [
    "vocab, merges = bpe_baseline(pretoken_counts, vocab_size, special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7d0c2190-2c80-45be-8c2b-8a123459ec65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T08:55:13.982320Z",
     "iopub.status.busy": "2025-12-31T08:55:13.982026Z",
     "iopub.status.idle": "2025-12-31T08:55:13.996317Z",
     "shell.execute_reply": "2025-12-31T08:55:13.992357Z",
     "shell.execute_reply.started": "2025-12-31T08:55:13.982294Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging pair (b'i', b'n') of count 3 to new token 257\n",
      "Merging pair (b' ', b'a') of count 3 to new token 258\n",
      "Merging pair (b't', b'in') of count 2 to new token 259\n",
      "Merging pair (b'tin', b'g') of count 2 to new token 260\n",
      "Merging pair (b's', b'ting') of count 2 to new token 261\n",
      "Merging pair (b'p', b'l') of count 2 to new token 262\n",
      "Merging pair (b'pl', b'a') of count 2 to new token 263\n",
      "Merging pair (b'pla', b'c') of count 2 to new token 264\n",
      "Merging pair (b'plac', b'e') of count 2 to new token 265\n",
      "Merging pair (b'e', b'sting') of count 2 to new token 266\n",
      "Merging pair (b'e', b'r') of count 2 to new token 267\n",
      "Merging pair (b' ', b'place') of count 2 to new token 268\n",
      "Merging pair (b' ', b'o') of count 2 to new token 269\n",
      "Merging pair (b'v', b'e') of count 1 to new token 270\n",
      "Merging pair (b've', b'd') of count 1 to new token 271\n",
      "Merging pair (b't', b'esting') of count 1 to new token 272\n",
      "Merging pair (b's', b'er') of count 1 to new token 273\n",
      "Merging pair (b'ser', b'ved') of count 1 to new token 274\n",
      "Merging pair (b'r', b'o') of count 1 to new token 275\n",
      "Merging pair (b'ro', b'n') of count 1 to new token 276\n",
      "Merging pair (b'ron', b't') of count 1 to new token 277\n",
      "Merging pair (b'r', b'esting') of count 1 to new token 278\n",
      "Merging pair (b'p', b'er') of count 1 to new token 279\n",
      "Merging pair (b'per', b's') of count 1 to new token 280\n",
      "Merging pair (b'o', b't') of count 1 to new token 281\n",
      "Merging pair (b'o', b'pers') of count 1 to new token 282\n",
      "Merging pair (b'o', b'opers') of count 1 to new token 283\n",
      "Merging pair (b'n', b'ot') of count 1 to new token 284\n",
      "Merging pair (b'l', b'l') of count 1 to new token 285\n",
      "Merging pair (b'i', b's') of count 1 to new token 286\n",
      "Merging pair (b'i', b'f') of count 1 to new token 287\n",
      "Merging pair (b'if', b'e') of count 1 to new token 288\n",
      "Merging pair (b'h', b'oopers') of count 1 to new token 289\n",
      "Merging pair (b'f', b'ront') of count 1 to new token 290\n",
      "Merging pair (b'b', b'served') of count 1 to new token 291\n",
      "Merging pair (b'L', b'ife') of count 1 to new token 292\n",
      "Merging pair (b'E', b'd') of count 1 to new token 293\n",
      "Merging pair (b'.', b'\"') of count 1 to new token 294\n",
      "Merging pair (b' o', b'f') of count 1 to new token 295\n",
      "Merging pair (b' o', b'bserved') of count 1 to new token 296\n",
      "Merging pair (b' a', b'll') of count 1 to new token 297\n",
      "Merging pair (b' ', b'testing') of count 1 to new token 298\n",
      "Merging pair (b' ', b'resting') of count 1 to new token 299\n",
      "Merging pair (b' ', b'not') of count 1 to new token 300\n",
      "Merging pair (b' ', b'is') of count 1 to new token 301\n",
      "Merging pair (b' ', b'in') of count 1 to new token 302\n",
      "Merging pair (b' ', b'hoopers') of count 1 to new token 303\n",
      "Merging pair (b' ', b'front') of count 1 to new token 304\n",
      "Merging pair (b' ', b'\"') of count 1 to new token 305\n",
      "Cannot merge further as token pairs have run out. Actual vocab size: 306 Expected: 1000\n"
     ]
    }
   ],
   "source": [
    "vocab, merges = bpe_time_suboptimal(pretoken_counts, vocab_size, special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59e1a9bc-8fa4-47bc-a48c-4dd3906e8b50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T01:37:53.131531Z",
     "iopub.status.busy": "2026-01-02T01:37:53.130663Z",
     "iopub.status.idle": "2026-01-02T01:37:53.141784Z",
     "shell.execute_reply": "2026-01-02T01:37:53.140187Z",
     "shell.execute_reply.started": "2026-01-02T01:37:53.131494Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging pair (b'i', b'n') of count 8 to new token 257\n",
      "Merging pair (b'in', b'g') of count 7 to new token 258\n",
      "Merging pair (b'o', b'u') of count 4 to new token 259\n",
      "Merging pair (b'o', b't') of count 4 to new token 260\n",
      "Merging pair (b'e', b's') of count 4 to new token 261\n",
      "Merging pair (b' ', b'a') of count 4 to new token 262\n",
      "Merging pair (b'ou', b'r') of count 3 to new token 263\n",
      "Merging pair (b'y', b'our') of count 2 to new token 264\n",
      "Merging pair (b'u', b'ot') of count 2 to new token 265\n",
      "Merging pair (b't', b'ing') of count 2 to new token 266\n",
      "Merging pair (b't', b'i') of count 2 to new token 267\n",
      "Merging pair (b't', b'h') of count 2 to new token 268\n",
      "Merging pair (b'q', b'uot') of count 2 to new token 269\n",
      "Merging pair (b'p', b'l') of count 2 to new token 270\n",
      "Merging pair (b'pl', b'a') of count 2 to new token 271\n",
      "Merging pair (b'pla', b'c') of count 2 to new token 272\n",
      "Merging pair (b'plac', b'e') of count 2 to new token 273\n",
      "Merging pair (b'o', b'p') of count 2 to new token 274\n",
      "Merging pair (b'n', b'd') of count 2 to new token 275\n",
      "Merging pair (b'es', b'ting') of count 2 to new token 276\n",
      "Merging pair (b'e', b'g') of count 2 to new token 277\n",
      "Merging pair (b'd', b'e') of count 2 to new token 278\n",
      "Merging pair (b' ', b'your') of count 2 to new token 279\n",
      "Merging pair (b' ', b'r') of count 2 to new token 280\n",
      "Merging pair (b' ', b'quot') of count 2 to new token 281\n",
      "Merging pair (b' ', b'place') of count 2 to new token 282\n",
      "Merging pair (b' ', b'f') of count 2 to new token 283\n",
      "Merging pair (b'w', b'ing') of count 1 to new token 284\n",
      "Merging pair (b'v', b'e') of count 1 to new token 285\n",
      "Merging pair (b'u', b'n') of count 1 to new token 286\n",
      "Merging pair (b'un', b'n') of count 1 to new token 287\n",
      "Merging pair (b'unn', b'ing') of count 1 to new token 288\n",
      "Merging pair (b'u', b'e') of count 1 to new token 289\n",
      "Merging pair (b'u', b'de') of count 1 to new token 290\n",
      "Merging pair (b'ti', b't') of count 1 to new token 291\n",
      "Merging pair (b'tit', b'ude') of count 1 to new token 292\n",
      "Merging pair (b'ti', b'd') of count 1 to new token 293\n",
      "Merging pair (b'tid', b'ue') of count 1 to new token 294\n",
      "Merging pair (b'th', b'e') of count 1 to new token 295\n",
      "Merging pair (b't', b'tidue') of count 1 to new token 296\n",
      "Merging pair (b't', b'op') of count 1 to new token 297\n",
      "Merging pair (b't', b'esting') of count 1 to new token 298\n",
      "Merging pair (b's', b'p') of count 1 to new token 299\n"
     ]
    }
   ],
   "source": [
    "vocab_n, merges_n = bpe_time_optimized(pretoken_counts, vocab_size, special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54c67bd2-0040-4ce3-83b5-076efb1acde5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T16:53:27.052092Z",
     "iopub.status.busy": "2026-01-01T16:53:27.051768Z",
     "iopub.status.idle": "2026-01-01T16:53:27.059483Z",
     "shell.execute_reply": "2026-01-01T16:53:27.057623Z",
     "shell.execute_reply.started": "2026-01-01T16:53:27.052064Z"
    }
   },
   "outputs": [],
   "source": [
    "assert merges == merges_n, \"Merges from optimized BPE doesn't match that from baseline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3241d9d-c71c-47a8-9997-46bc165e1d4c",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2026-01-01T16:53:31.976467Z",
     "iopub.status.busy": "2026-01-01T16:53:31.976110Z",
     "iopub.status.idle": "2026-01-01T16:53:31.999894Z",
     "shell.execute_reply": "2026-01-01T16:53:31.998746Z",
     "shell.execute_reply.started": "2026-01-01T16:53:31.976435Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: b'\\x00',\n",
       " 1: b'\\x01',\n",
       " 2: b'\\x02',\n",
       " 3: b'\\x03',\n",
       " 4: b'\\x04',\n",
       " 5: b'\\x05',\n",
       " 6: b'\\x06',\n",
       " 7: b'\\x07',\n",
       " 8: b'\\x08',\n",
       " 9: b'\\t',\n",
       " 10: b'\\n',\n",
       " 11: b'\\x0b',\n",
       " 12: b'\\x0c',\n",
       " 13: b'\\r',\n",
       " 14: b'\\x0e',\n",
       " 15: b'\\x0f',\n",
       " 16: b'\\x10',\n",
       " 17: b'\\x11',\n",
       " 18: b'\\x12',\n",
       " 19: b'\\x13',\n",
       " 20: b'\\x14',\n",
       " 21: b'\\x15',\n",
       " 22: b'\\x16',\n",
       " 23: b'\\x17',\n",
       " 24: b'\\x18',\n",
       " 25: b'\\x19',\n",
       " 26: b'\\x1a',\n",
       " 27: b'\\x1b',\n",
       " 28: b'\\x1c',\n",
       " 29: b'\\x1d',\n",
       " 30: b'\\x1e',\n",
       " 31: b'\\x1f',\n",
       " 32: b' ',\n",
       " 33: b'!',\n",
       " 34: b'\"',\n",
       " 35: b'#',\n",
       " 36: b'$',\n",
       " 37: b'%',\n",
       " 38: b'&',\n",
       " 39: b\"'\",\n",
       " 40: b'(',\n",
       " 41: b')',\n",
       " 42: b'*',\n",
       " 43: b'+',\n",
       " 44: b',',\n",
       " 45: b'-',\n",
       " 46: b'.',\n",
       " 47: b'/',\n",
       " 48: b'0',\n",
       " 49: b'1',\n",
       " 50: b'2',\n",
       " 51: b'3',\n",
       " 52: b'4',\n",
       " 53: b'5',\n",
       " 54: b'6',\n",
       " 55: b'7',\n",
       " 56: b'8',\n",
       " 57: b'9',\n",
       " 58: b':',\n",
       " 59: b';',\n",
       " 60: b'<',\n",
       " 61: b'=',\n",
       " 62: b'>',\n",
       " 63: b'?',\n",
       " 64: b'@',\n",
       " 65: b'A',\n",
       " 66: b'B',\n",
       " 67: b'C',\n",
       " 68: b'D',\n",
       " 69: b'E',\n",
       " 70: b'F',\n",
       " 71: b'G',\n",
       " 72: b'H',\n",
       " 73: b'I',\n",
       " 74: b'J',\n",
       " 75: b'K',\n",
       " 76: b'L',\n",
       " 77: b'M',\n",
       " 78: b'N',\n",
       " 79: b'O',\n",
       " 80: b'P',\n",
       " 81: b'Q',\n",
       " 82: b'R',\n",
       " 83: b'S',\n",
       " 84: b'T',\n",
       " 85: b'U',\n",
       " 86: b'V',\n",
       " 87: b'W',\n",
       " 88: b'X',\n",
       " 89: b'Y',\n",
       " 90: b'Z',\n",
       " 91: b'[',\n",
       " 92: b'\\\\',\n",
       " 93: b']',\n",
       " 94: b'^',\n",
       " 95: b'_',\n",
       " 96: b'`',\n",
       " 97: b'a',\n",
       " 98: b'b',\n",
       " 99: b'c',\n",
       " 100: b'd',\n",
       " 101: b'e',\n",
       " 102: b'f',\n",
       " 103: b'g',\n",
       " 104: b'h',\n",
       " 105: b'i',\n",
       " 106: b'j',\n",
       " 107: b'k',\n",
       " 108: b'l',\n",
       " 109: b'm',\n",
       " 110: b'n',\n",
       " 111: b'o',\n",
       " 112: b'p',\n",
       " 113: b'q',\n",
       " 114: b'r',\n",
       " 115: b's',\n",
       " 116: b't',\n",
       " 117: b'u',\n",
       " 118: b'v',\n",
       " 119: b'w',\n",
       " 120: b'x',\n",
       " 121: b'y',\n",
       " 122: b'z',\n",
       " 123: b'{',\n",
       " 124: b'|',\n",
       " 125: b'}',\n",
       " 126: b'~',\n",
       " 127: b'\\x7f',\n",
       " 128: b'\\x80',\n",
       " 129: b'\\x81',\n",
       " 130: b'\\x82',\n",
       " 131: b'\\x83',\n",
       " 132: b'\\x84',\n",
       " 133: b'\\x85',\n",
       " 134: b'\\x86',\n",
       " 135: b'\\x87',\n",
       " 136: b'\\x88',\n",
       " 137: b'\\x89',\n",
       " 138: b'\\x8a',\n",
       " 139: b'\\x8b',\n",
       " 140: b'\\x8c',\n",
       " 141: b'\\x8d',\n",
       " 142: b'\\x8e',\n",
       " 143: b'\\x8f',\n",
       " 144: b'\\x90',\n",
       " 145: b'\\x91',\n",
       " 146: b'\\x92',\n",
       " 147: b'\\x93',\n",
       " 148: b'\\x94',\n",
       " 149: b'\\x95',\n",
       " 150: b'\\x96',\n",
       " 151: b'\\x97',\n",
       " 152: b'\\x98',\n",
       " 153: b'\\x99',\n",
       " 154: b'\\x9a',\n",
       " 155: b'\\x9b',\n",
       " 156: b'\\x9c',\n",
       " 157: b'\\x9d',\n",
       " 158: b'\\x9e',\n",
       " 159: b'\\x9f',\n",
       " 160: b'\\xa0',\n",
       " 161: b'\\xa1',\n",
       " 162: b'\\xa2',\n",
       " 163: b'\\xa3',\n",
       " 164: b'\\xa4',\n",
       " 165: b'\\xa5',\n",
       " 166: b'\\xa6',\n",
       " 167: b'\\xa7',\n",
       " 168: b'\\xa8',\n",
       " 169: b'\\xa9',\n",
       " 170: b'\\xaa',\n",
       " 171: b'\\xab',\n",
       " 172: b'\\xac',\n",
       " 173: b'\\xad',\n",
       " 174: b'\\xae',\n",
       " 175: b'\\xaf',\n",
       " 176: b'\\xb0',\n",
       " 177: b'\\xb1',\n",
       " 178: b'\\xb2',\n",
       " 179: b'\\xb3',\n",
       " 180: b'\\xb4',\n",
       " 181: b'\\xb5',\n",
       " 182: b'\\xb6',\n",
       " 183: b'\\xb7',\n",
       " 184: b'\\xb8',\n",
       " 185: b'\\xb9',\n",
       " 186: b'\\xba',\n",
       " 187: b'\\xbb',\n",
       " 188: b'\\xbc',\n",
       " 189: b'\\xbd',\n",
       " 190: b'\\xbe',\n",
       " 191: b'\\xbf',\n",
       " 192: b'\\xc0',\n",
       " 193: b'\\xc1',\n",
       " 194: b'\\xc2',\n",
       " 195: b'\\xc3',\n",
       " 196: b'\\xc4',\n",
       " 197: b'\\xc5',\n",
       " 198: b'\\xc6',\n",
       " 199: b'\\xc7',\n",
       " 200: b'\\xc8',\n",
       " 201: b'\\xc9',\n",
       " 202: b'\\xca',\n",
       " 203: b'\\xcb',\n",
       " 204: b'\\xcc',\n",
       " 205: b'\\xcd',\n",
       " 206: b'\\xce',\n",
       " 207: b'\\xcf',\n",
       " 208: b'\\xd0',\n",
       " 209: b'\\xd1',\n",
       " 210: b'\\xd2',\n",
       " 211: b'\\xd3',\n",
       " 212: b'\\xd4',\n",
       " 213: b'\\xd5',\n",
       " 214: b'\\xd6',\n",
       " 215: b'\\xd7',\n",
       " 216: b'\\xd8',\n",
       " 217: b'\\xd9',\n",
       " 218: b'\\xda',\n",
       " 219: b'\\xdb',\n",
       " 220: b'\\xdc',\n",
       " 221: b'\\xdd',\n",
       " 222: b'\\xde',\n",
       " 223: b'\\xdf',\n",
       " 224: b'\\xe0',\n",
       " 225: b'\\xe1',\n",
       " 226: b'\\xe2',\n",
       " 227: b'\\xe3',\n",
       " 228: b'\\xe4',\n",
       " 229: b'\\xe5',\n",
       " 230: b'\\xe6',\n",
       " 231: b'\\xe7',\n",
       " 232: b'\\xe8',\n",
       " 233: b'\\xe9',\n",
       " 234: b'\\xea',\n",
       " 235: b'\\xeb',\n",
       " 236: b'\\xec',\n",
       " 237: b'\\xed',\n",
       " 238: b'\\xee',\n",
       " 239: b'\\xef',\n",
       " 240: b'\\xf0',\n",
       " 241: b'\\xf1',\n",
       " 242: b'\\xf2',\n",
       " 243: b'\\xf3',\n",
       " 244: b'\\xf4',\n",
       " 245: b'\\xf5',\n",
       " 246: b'\\xf6',\n",
       " 247: b'\\xf7',\n",
       " 248: b'\\xf8',\n",
       " 249: b'\\xf9',\n",
       " 250: b'\\xfa',\n",
       " 251: b'\\xfb',\n",
       " 252: b'\\xfc',\n",
       " 253: b'\\xfd',\n",
       " 254: b'\\xfe',\n",
       " 255: b'\\xff',\n",
       " 256: b'<|endoftext|>',\n",
       " 257: b'in',\n",
       " 258: b'ing',\n",
       " 259: b'ou',\n",
       " 260: b'ot',\n",
       " 261: b'es',\n",
       " 262: b' a',\n",
       " 263: b'our',\n",
       " 264: b'your',\n",
       " 265: b'uot',\n",
       " 266: b'ting',\n",
       " 267: b'ti',\n",
       " 268: b'th',\n",
       " 269: b'quot',\n",
       " 270: b'pl',\n",
       " 271: b'pla',\n",
       " 272: b'plac',\n",
       " 273: b'place',\n",
       " 274: b'op',\n",
       " 275: b'nd',\n",
       " 276: b'esting',\n",
       " 277: b'eg',\n",
       " 278: b'de',\n",
       " 279: b' your',\n",
       " 280: b' r',\n",
       " 281: b' quot',\n",
       " 282: b' place',\n",
       " 283: b' f',\n",
       " 284: b'wing',\n",
       " 285: b've',\n",
       " 286: b'un',\n",
       " 287: b'unn',\n",
       " 288: b'unning',\n",
       " 289: b'ue',\n",
       " 290: b'ude',\n",
       " 291: b'tit',\n",
       " 292: b'titude',\n",
       " 293: b'tid',\n",
       " 294: b'tidue',\n",
       " 295: b'the',\n",
       " 296: b'ttidue',\n",
       " 297: b'top',\n",
       " 298: b'testing',\n",
       " 299: b'sp'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7be5258-1b77-4ac9-9081-83f64c4b6cb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T16:55:55.091251Z",
     "iopub.status.busy": "2026-01-01T16:55:55.090881Z",
     "iopub.status.idle": "2026-01-01T16:55:55.106836Z",
     "shell.execute_reply": "2026-01-01T16:55:55.105427Z",
     "shell.execute_reply.started": "2026-01-01T16:55:55.091218Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'i', b'n'),\n",
       " (b'in', b'g'),\n",
       " (b'o', b'u'),\n",
       " (b'o', b't'),\n",
       " (b'e', b's'),\n",
       " (b' ', b'a'),\n",
       " (b'ou', b'r'),\n",
       " (b'y', b'our'),\n",
       " (b'u', b'ot'),\n",
       " (b't', b'ing'),\n",
       " (b't', b'i'),\n",
       " (b't', b'h'),\n",
       " (b'q', b'uot'),\n",
       " (b'p', b'l'),\n",
       " (b'pl', b'a'),\n",
       " (b'pla', b'c'),\n",
       " (b'plac', b'e'),\n",
       " (b'o', b'p'),\n",
       " (b'n', b'd'),\n",
       " (b'es', b'ting'),\n",
       " (b'e', b'g'),\n",
       " (b'd', b'e'),\n",
       " (b' ', b'your'),\n",
       " (b' ', b'r'),\n",
       " (b' ', b'quot'),\n",
       " (b' ', b'place'),\n",
       " (b' ', b'f'),\n",
       " (b'w', b'ing'),\n",
       " (b'v', b'e'),\n",
       " (b'u', b'n'),\n",
       " (b'un', b'n'),\n",
       " (b'unn', b'ing'),\n",
       " (b'u', b'e'),\n",
       " (b'u', b'de'),\n",
       " (b'ti', b't'),\n",
       " (b'tit', b'ude'),\n",
       " (b'ti', b'd'),\n",
       " (b'tid', b'ue'),\n",
       " (b'th', b'e'),\n",
       " (b't', b'tidue'),\n",
       " (b't', b'op'),\n",
       " (b't', b'esting'),\n",
       " (b's', b'p')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b49a7f-4755-4e67-8d7a-2d832740ee67",
   "metadata": {},
   "source": [
    "# Tokenizer encoding and decoding logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2412d258-7fbf-4b92-91e0-8d0a8d3c61af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T05:33:40.237517Z",
     "iopub.status.busy": "2026-01-02T05:33:40.237210Z",
     "iopub.status.idle": "2026-01-02T05:33:40.257657Z",
     "shell.execute_reply": "2026-01-02T05:33:40.256322Z",
     "shell.execute_reply.started": "2026-01-02T05:33:40.237491Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from itertools import chain\n",
    "\n",
    "class Tokenizer:\n",
    "    '''\n",
    "    Tokenizer encodes given text to token sequence and decodes given token\n",
    "    sequence to text.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, vocab: dict[int, bytes], merges: list[tuple[bytes,\n",
    "                                                                   bytes]],\n",
    "                 special_tokens: list[str] | None =None) -> None:\n",
    "        '''\n",
    "        Spec:\n",
    "        Keep vocab for decoding token sequence to text.\n",
    "        Keep merges for encoding text to token sequence.\n",
    "        Need a way to look up a token's index given its bytes representation,\n",
    "        so need to build a reverse bytes -> int mapping from vocab.\n",
    "        For special tokens, first filter out those already existed in the\n",
    "        reversed index built from vocab, then assign new id to the remaining\n",
    "        ones and add mapping to both vocab and the reversed index, starting\n",
    "        from |vocab|.\n",
    "        '''\n",
    "        self.vocab  = vocab\n",
    "        # Merges need to proceed in the same order as merged pair creation so\n",
    "        # use index as ordering indicator -- A pass to find token pair to merge\n",
    "        # in a pretoken will need to find the pair of smallest index in mapping\n",
    "        # below.\n",
    "        self.merges: dict[tuple[bytes, bytes], int] = { p: idx for idx, p in\n",
    "                                                       enumerate(merges) }\n",
    "        # the reverse bytes -> int mapping from given vocab\n",
    "        self.bytes_to_token: dict[bytes, int] = { v: k for k, v in\n",
    "                                                 vocab.items() }\n",
    "        self.pretoken_to_tokens: dict[str, list[int]] = {}\n",
    "        self.pretokenize_pat = PRE_TOKENIZE_PAT\n",
    "        self.special_tokens_pat = None\n",
    "        special_tokens_regexes = []\n",
    "        if isinstance(special_tokens, list) :\n",
    "            for t in sorted(special_tokens, reverse=True):\n",
    "                # to capture each special token in pretokenization\n",
    "                special_tokens_regexes.append(re.escape(t))\n",
    "                tb = t.encode(UTF8)\n",
    "                if tb not in self.bytes_to_token:\n",
    "                    new_token_id = len(vocab)\n",
    "                    vocab[new_token_id] = tb\n",
    "                    self.bytes_to_token[tb] = new_token_id\n",
    "                self.pretoken_to_tokens[t] = [self.bytes_to_token[tb]]\n",
    "\n",
    "            if special_tokens_regexes:\n",
    "                self.special_tokens_pat = re.compile('|'.join(special_tokens_regexes))\n",
    "\n",
    "    @classmethod\n",
    "    def from_files(cls, vocab_filepath: str, merges_filepath: str,\n",
    "                   special_tokens: list[str] | None = None) -> 'Tokenizer':\n",
    "        # TODO what does the content look like in vocab_filepath and merges_filepath?\n",
    "        with open(vocab_filepath) as f_vocab:\n",
    "            with open(merges_filepath) as f_merges:\n",
    "                # FIXME clawning for now\n",
    "                vocab = json.load(f_vocab)\n",
    "                merges = json.load(f_merges)\n",
    "                return cls(vocab, merges, special_tokens)\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        '''\n",
    "        Spec:\n",
    "\n",
    "        Pretokenize text. The result shall be:\n",
    "        1. A mapping of pretokens -> list[int], whose value set to None\n",
    "        2. A list of pretokens in order of given text.\n",
    "        NOTE before pretokenization we must split text by given special\n",
    "        tokens if any to avoid tokenizing the special tokens.\n",
    "\n",
    "        For each pretoken in the mapping, find corresponding token list by\n",
    "        merging the pretoken's byte-level presentation.\n",
    "\n",
    "        Finally compose the return value by initing an empty list l, then for\n",
    "        each pretoken in the list from point 2 above, extend l to\n",
    "        include the corresponding token list.\n",
    "\n",
    "        If encoding is called fairly frequently, it can be desirable for the\n",
    "        Toekenizer instance to maintain a pretoken -> token list mapping as an\n",
    "        attribute and use resulting cache effect to speed up encoding.\n",
    "        '''\n",
    "        txt_and_spt_pairs_iter = [(text, '')]\n",
    "        if self.special_tokens_pat:\n",
    "            # Python has handy, comprehensive builtins for dealing w/ iteration:\n",
    "            # https://docs.python.org/3/library/itertools.html\n",
    "            special_tokens_iter = chain(\n",
    "                map(\n",
    "                    lambda m: text[m.start() : m.end()],\n",
    "                    re.finditer(self.special_tokens_pat, text)),\n",
    "                # Below is necessary tomake the loop cover the last txt piece\n",
    "                # after splitting the given text by special tokens\n",
    "                [''],\n",
    "            )\n",
    "            txt_and_spt_pairs_iter = zip(\n",
    "                re.splititer(self.special_tokens_pat, text),\n",
    "                special_tokens_iter,\n",
    "            )\n",
    "\n",
    "        tokens = []\n",
    "        for txt, spt in txt_and_spt_pairs_iter:\n",
    "            # TODO what if txt is empty str?\n",
    "            for m in re.finditer(self.pretokenize_pat, txt):\n",
    "                pt = txt[m.start():m.end()]\n",
    "                if pt in self.pretoken_to_tokens:\n",
    "                    tokens.extend(self.pretoken_to_tokens[pt])\n",
    "                    continue\n",
    "                # TODO pt not in pretoken -> token list mapping; Compute token list\n",
    "                # and cache. Start merging from byte-level tokens\n",
    "                pt_tokens = [bytes([b]) for b in pt.encode(UTF8)]\n",
    "                # Enumerate the pairs in ptb and replace, until no replacement can\n",
    "                # be found. TODO finally cache the pretoken -> token list entry\n",
    "                # to self.pretoken_to_tokens\n",
    "                # Edge case: pretoken contains only 1 byte\n",
    "                while True:\n",
    "                    merged_p, merged_p_order = None, math.inf\n",
    "                    for p in zip(pt_tokens, pt_tokens[1:]):\n",
    "                        p_order = self.merges.get(p, math.inf)\n",
    "                        if p_order is not math.inf and p_order < merged_p_order:\n",
    "                            merged_p = p\n",
    "                            merged_p_order = p_order\n",
    "                    # Found pair to merge or it is still None. If it is latter\n",
    "                    # break, as no new merged pair is found; Otherwise replace token list\n",
    "                    # w/ one w/ the merged token.\n",
    "                    if merged_p is None:\n",
    "                        break\n",
    "                    merged_token = b''.join(merged_p)\n",
    "                    print(f'Processing pretoken \"{pt}\": Merging {merged_p} into {merged_token}')\n",
    "                    pt_tokens_w_merged_p = []\n",
    "                    idx, ln = 0, len(pt_tokens)\n",
    "                    while idx < ln:\n",
    "                        if idx < ln-1 and pt_tokens[idx] == merged_p[0] and pt_tokens[idx+1] == merged_p[1]:\n",
    "                            pt_tokens_w_merged_p.append(merged_token)\n",
    "                            idx+=2\n",
    "                        else:\n",
    "                            pt_tokens_w_merged_p.append(pt_tokens[idx])\n",
    "                            idx+=1\n",
    "                    # prepare for next merge run\n",
    "                    pt_tokens = pt_tokens_w_merged_p\n",
    "                # Now look up the id of final tokens given their bytes representation\n",
    "                pt_tokens = [ self.bytes_to_token[b] for b in pt_tokens ]\n",
    "                self.pretoken_to_tokens[pt] = pt_tokens\n",
    "                # finally extend the merged tokens to final token list\n",
    "                tokens.extend(pt_tokens)\n",
    "            # append id of special token to token list\n",
    "            if spt != '':\n",
    "                tokens.extend(self.pretoken_to_tokens[spt])\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def encode_iterable(self, iterable: Iterable[str]) -> Iterator[int]:\n",
    "        '''\n",
    "        More on generator see:\n",
    "        - https://stackoverflow.com/a/1756156\n",
    "        - https://wiki.python.org/moin/Generators\n",
    "        Spec:\n",
    "\n",
    "        Iterate the given iterable - For each txt in iterable:\n",
    "            tokens = self.encode(txt)\n",
    "            for t in tokens:\n",
    "                yield t\n",
    "        '''\n",
    "        for txt in iterable:\n",
    "            yield from self.encode(txt)\n",
    "\n",
    "\n",
    "    def decode(self, ids: list[int]) -> str:\n",
    "        '''\n",
    "        Spec:\n",
    "        Start w/ an empty byte array.\n",
    "        For each token id in ids:\n",
    "            Look up vocab to get the bytes for token identified by id\n",
    "            Put the bytes into byte array\n",
    "        Decode byte array directly to str (feasible?)\n",
    "        '''\n",
    "        b = bytearray()\n",
    "        for t_id in ids:\n",
    "            b.extend(self.vocab[t_id])\n",
    "        return b.decode(encoding=UTF8, errors='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7cb9aea-82be-47e4-84a0-2f21d328a974",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T01:49:58.241316Z",
     "iopub.status.busy": "2026-01-02T01:49:58.240946Z",
     "iopub.status.idle": "2026-01-02T01:49:58.254595Z",
     "shell.execute_reply": "2026-01-02T01:49:58.252472Z",
     "shell.execute_reply.started": "2026-01-02T01:49:58.241287Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(73, b'I'),\n",
       " (32, b' '),\n",
       " (108, b'l'),\n",
       " (111, b'o'),\n",
       " (285, b've'),\n",
       " (300, b'<|endofprompt|>'),\n",
       " (104, b'h'),\n",
       " (111, b'o'),\n",
       " (274, b'op'),\n",
       " (258, b'ing'),\n",
       " (32, b' '),\n",
       " (230, b'\\xe6'),\n",
       " (137, b'\\x89'),\n",
       " (147, b'\\x93'),\n",
       " (231, b'\\xe7'),\n",
       " (144, b'\\x90'),\n",
       " (131, b'\\x83'),\n",
       " (240, b'\\xf0'),\n",
       " (159, b'\\x9f'),\n",
       " (143, b'\\x8f'),\n",
       " (128, b'\\x80'),\n",
       " (33, b'!'),\n",
       " (256, b'<|endoftext|>'),\n",
       " (104, b'h'),\n",
       " (111, b'o'),\n",
       " (274, b'op'),\n",
       " (32, b' '),\n",
       " (105, b'i'),\n",
       " (115, b's'),\n",
       " (32, b' '),\n",
       " (108, b'l'),\n",
       " (105, b'i'),\n",
       " (102, b'f'),\n",
       " (101, b'e'),\n",
       " (32, b' '),\n",
       " (121, b'y'),\n",
       " (111, b'o'),\n",
       " (121, b'y'),\n",
       " (111, b'o'),\n",
       " (256, b'<|endoftext|>'),\n",
       " (84, b'T'),\n",
       " (104, b'h'),\n",
       " (101, b'e'),\n",
       " (32, b' '),\n",
       " (108, b'l'),\n",
       " (97, b'a'),\n",
       " (115, b's'),\n",
       " (116, b't'),\n",
       " (32, b' '),\n",
       " (116, b't'),\n",
       " (120, b'x'),\n",
       " (116, b't'),\n",
       " (32, b' '),\n",
       " (112, b'p'),\n",
       " (105, b'i'),\n",
       " (101, b'e'),\n",
       " (99, b'c'),\n",
       " (101, b'e')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens = ['<|endoftext|>', '<|endofprompt|>']\n",
    "\n",
    "tk = Tokenizer(vocab_n, merges_n, special_tokens)\n",
    "\n",
    "txt_to_encode = 'I love<|endofprompt|>hooping !<|endoftext|>hoop is life yoyo<|endoftext|>The last txt piece'\n",
    "#txt_to_encode = 'I love sexy white girls.'\n",
    "\n",
    "[ (i, vocab_n[i]) for i in tk.encode(txt_to_encode) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b00b268-fe71-4165-ad86-eaaa7487ff22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T01:50:29.529760Z",
     "iopub.status.busy": "2026-01-02T01:50:29.529420Z",
     "iopub.status.idle": "2026-01-02T01:50:29.547148Z",
     "shell.execute_reply": "2026-01-02T01:50:29.544919Z",
     "shell.execute_reply.started": "2026-01-02T01:50:29.529730Z"
    }
   },
   "outputs": [],
   "source": [
    "assert tk.decode(tk.encode(txt_to_encode)) == txt_to_encode, 'Tokenizer encode decode trip failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0ed4cff-8b94-4ceb-9272-0610a7cec76b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T01:50:31.002722Z",
     "iopub.status.busy": "2026-01-02T01:50:31.002377Z",
     "iopub.status.idle": "2026-01-02T01:50:31.010479Z",
     "shell.execute_reply": "2026-01-02T01:50:31.009337Z",
     "shell.execute_reply.started": "2026-01-02T01:50:31.002691Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "txt_list = [\n",
    "    \"Today is raining, \",\n",
    "    \"plus it is foggy.\",\n",
    "]\n",
    "\n",
    "tks_from_gen = [ t for t in tk.encode_iterable(txt_list) ]\n",
    "\n",
    "tks_from_encode = tk.encode(''.join(txt_list))\n",
    "\n",
    "assert tks_from_gen == tks_from_encode, \"Tokens generated by encode_iterable don't match that from encode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b2285354-948c-41ba-b6e7-103405fd914d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T18:43:13.574935Z",
     "iopub.status.busy": "2026-01-01T18:43:13.574599Z",
     "iopub.status.idle": "2026-01-01T18:43:13.589342Z",
     "shell.execute_reply": "2026-01-01T18:43:13.587958Z",
     "shell.execute_reply.started": "2026-01-01T18:43:13.574907Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "piece: \"I am running \" special token = \"<|endofprompt|>\"\n",
      "piece: \"and hooping !\" special token = \"<|endoftext|>\"\n",
      "piece: \"some foobar text yoyo\" special token = \"<|endofprompt|>\"\n",
      "piece: \"some other txt heyhey\" special token = \"<|endoftext|>\"\n",
      "piece: \"The last txt piece\" special token = \"\"\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "'''\n",
    "Split given txt by given special tokens.\n",
    "Then map each splitted piece to the special token which comes right after the piece.\n",
    "Pretokenize the piece as normal, resulting a list of token ids.\n",
    "Append the token id of the following special token to the list.\n",
    "'''\n",
    "pieces = re.splititer(tk.special_tokens_pat, txt_to_encode)\n",
    "spts = map(lambda m: txt_to_encode[m.start() : m.end()], re.finditer(tk.special_tokens_pat, txt_to_encode))\n",
    "spts = chain(spts, [''])\n",
    "for piece, spt in zip(pieces, spts):\n",
    "    print(f'piece: \"{piece}\" special token = \"{spt}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a892b1b-21b3-4d65-bc7d-3155fb3e3743",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2026-01-02T04:08:32.729966Z",
     "iopub.status.busy": "2026-01-02T04:08:32.729628Z",
     "iopub.status.idle": "2026-01-02T04:08:35.390606Z",
     "shell.execute_reply": "2026-01-02T04:08:35.389346Z",
     "shell.execute_reply.started": "2026-01-02T04:08:32.729939Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/doobdoob/projects/stanford.css336/assignment1-basics/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/doobdoob/projects/stanford.css336/assignment1-basics/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/doobdoob/projects/stanford.css336/assignment1-basics/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 758, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/doobdoob/projects/stanford.css336/assignment1-basics/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.11/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.11/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.11/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/doobdoob/projects/stanford.css336/assignment1-basics/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n",
      "    await self.dispatch_shell(msg, subshell_id=subshell_id)\n",
      "  File \"/Users/doobdoob/projects/stanford.css336/assignment1-basics/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/doobdoob/projects/stanford.css336/assignment1-basics/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/doobdoob/projects/stanford.css336/assignment1-basics/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/doobdoob/projects/stanford.css336/assignment1-basics/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/doobdoob/projects/stanford.css336/assignment1-basics/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/doobdoob/projects/stanford.css336/assignment1-basics/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3123, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/doobdoob/projects/stanford.css336/assignment1-basics/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3178, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/doobdoob/projects/stanford.css336/assignment1-basics/.venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/doobdoob/projects/stanford.css336/assignment1-basics/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/doobdoob/projects/stanford.css336/assignment1-basics/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3641, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/doobdoob/projects/stanford.css336/assignment1-basics/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3701, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/gd/4_kkyrbj0bdbf6z97lft30hm0000gn/T/ipykernel_64530/1717734806.py\", line 1, in <module>\n",
      "    from tests.test_tokenizer import get_tokenizer_from_vocab_merges_path\n",
      "  File \"/Users/doobdoob/projects/stanford.css336/assignment1-basics/tests/test_tokenizer.py\", line 12, in <module>\n",
      "    from .adapters import get_tokenizer\n",
      "  File \"/Users/doobdoob/projects/stanford.css336/assignment1-basics/tests/adapters.py\", line 8, in <module>\n",
      "    import torch\n",
      "  File \"/Users/doobdoob/projects/stanford.css336/assignment1-basics/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/doobdoob/projects/stanford.css336/assignment1-basics/.venv/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/doobdoob/projects/stanford.css336/assignment1-basics/.venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/doobdoob/projects/stanford.css336/assignment1-basics/.venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/doobdoob/projects/stanford.css336/assignment1-basics/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/doobdoob/projects/stanford.css336/assignment1-basics/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "from tests.test_tokenizer import get_tokenizer_from_vocab_merges_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa221b7e-a350-4ba4-a854-48e116da4f1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T04:46:32.352864Z",
     "iopub.status.busy": "2026-01-02T04:46:32.347825Z",
     "iopub.status.idle": "2026-01-02T04:46:32.710330Z",
     "shell.execute_reply": "2026-01-02T04:46:32.707364Z",
     "shell.execute_reply.started": "2026-01-02T04:46:32.352767Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "FIXTURE_PATH = '../tests/fixtures/'\n",
    "_tk = get_tokenizer_from_vocab_merges_path(\n",
    "    vocab_path=Path(FIXTURE_PATH) / 'gpt2_vocab.json',\n",
    "    merges_path=Path(FIXTURE_PATH) / 'gpt2_merges.txt',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "982b10ca-a4de-489b-bfee-9cf0e159ae5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T04:46:32.879356Z",
     "iopub.status.busy": "2026-01-02T04:46:32.879024Z",
     "iopub.status.idle": "2026-01-02T04:46:32.943537Z",
     "shell.execute_reply": "2026-01-02T04:46:32.941302Z",
     "shell.execute_reply.started": "2026-01-02T04:46:32.879335Z"
    }
   },
   "outputs": [],
   "source": [
    "tk = Tokenizer(vocab=_tk.vocab, merges=_tk.merges.keys(), special_tokens=[\"<|endoftext|>\", \"<|endoftext|><|endoftext|>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc837810-43b5-4a2b-b93e-e03320e9d684",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T04:13:23.064154Z",
     "iopub.status.busy": "2026-01-02T04:13:23.063714Z",
     "iopub.status.idle": "2026-01-02T04:13:23.071736Z",
     "shell.execute_reply": "2026-01-02T04:13:23.070116Z",
     "shell.execute_reply.started": "2026-01-02T04:13:23.064121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2 vocab len = 50257, merges len = 50000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'<|endoftext|>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'gpt2 vocab len = {len(vocab)}, merges len = {len(merges)}')\n",
    "vocab[50256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5fe7d32-767d-4e9e-ab94-4e671da4290f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T04:19:59.855880Z",
     "iopub.status.busy": "2026-01-02T04:19:59.855548Z",
     "iopub.status.idle": "2026-01-02T04:19:59.897520Z",
     "shell.execute_reply": "2026-01-02T04:19:59.894996Z",
     "shell.execute_reply.started": "2026-01-02T04:19:59.855853Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(b'Hel', b'lo')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmerges\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mHel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlo\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mKeyError\u001b[39m: (b'Hel', b'lo')"
     ]
    }
   ],
   "source": [
    "merges[(b'Hel', b'lo')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "febfcf6e-322a-4b6f-b5ab-162c698bca12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T04:47:13.945148Z",
     "iopub.status.busy": "2026-01-02T04:47:13.944879Z",
     "iopub.status.idle": "2026-01-02T04:47:13.954779Z",
     "shell.execute_reply": "2026-01-02T04:47:13.953470Z",
     "shell.execute_reply.started": "2026-01-02T04:47:13.945126Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(15496, b'Hello'),\n",
       " (11, b','),\n",
       " (703, b' how'),\n",
       " (220, b' '),\n",
       " (50256, b'<|endoftext|>'),\n",
       " (50256, b'<|endoftext|>'),\n",
       " (389, b' are'),\n",
       " (345, b' you'),\n",
       " (30, b'?'),\n",
       " (50256, b'<|endoftext|>')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = \"Hello, how <|endoftext|><|endoftext|> are you?<|endoftext|>\"\n",
    "\n",
    "[ (i, vocab[i]) for i in tk.encode(txt) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72cce773-f7de-4e32-9665-ab01e041ad41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T04:16:41.242944Z",
     "iopub.status.busy": "2026-01-02T04:16:41.242456Z",
     "iopub.status.idle": "2026-01-02T04:16:41.255505Z",
     "shell.execute_reply": "2026-01-02T04:16:41.253658Z",
     "shell.execute_reply.started": "2026-01-02T04:16:41.242905Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_bytes = set(vocab.values())\n",
    "b'Hello' in vocab_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab3d1cde-4177-42c9-888e-8dba4e61b0b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T04:26:16.070740Z",
     "iopub.status.busy": "2026-01-02T04:26:16.070342Z",
     "iopub.status.idle": "2026-01-02T04:26:16.077328Z",
     "shell.execute_reply": "2026-01-02T04:26:16.075814Z",
     "shell.execute_reply.started": "2026-01-02T04:26:16.070708Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges[(b'e', b'l')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebfefe09-d3c8-4fb9-9beb-c247895b5c13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T05:32:25.774106Z",
     "iopub.status.busy": "2026-01-02T05:32:25.773771Z",
     "iopub.status.idle": "2026-01-02T05:32:25.781274Z",
     "shell.execute_reply": "2026-01-02T05:32:25.779744Z",
     "shell.execute_reply.started": "2026-01-02T05:32:25.774071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function sorted in module builtins:\n",
      "\n",
      "sorted(iterable, /, *, key=None, reverse=False)\n",
      "    Return a new list containing all items from the iterable in ascending order.\n",
      "\n",
      "    A custom key function can be supplied to customize the sort order, and the\n",
      "    reverse flag can be set to request the result in descending order.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faebe36-9540-47a3-ac05-c88a4334fd6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
